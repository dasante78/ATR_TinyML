{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BLwPMqcpmxi"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWmRw_S-vpuR"
   },
   "source": [
    "Goal: At begining reduce and sequnce from n to b where b < n alllowing for savings of O(n^2-b^2) runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlCdw4Knh0OB"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!pip install -U spacy\n",
    "!pip install  scipy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install datasets\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzHC5BtLi6Yu"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset,  DatasetDict, Dataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import wandb\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "login(token=\"ENTER TOKEN HERE\")\n",
    "wandb.login(key=\"ENTER KEY HERE\")\n",
    "device =  ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5RvMZjaMiqh"
   },
   "outputs": [],
   "source": [
    "class Language_Model(nn.Module):\n",
    "  def __init__(self, base_llm_model, forget_layer,choice_config=None ):\n",
    "    super().__init__()\n",
    "    self.base_llm_model = base_llm_model\n",
    "    self.forget_layer = forget_layer\n",
    "    self.choice_config = choice_config\n",
    "\n",
    "  def apply_mask(self, input_ids, mask, mask_temp, padding_value=0, attention =False):\n",
    "        \"\"\"\n",
    "        Applies a forgetting mask to input_ids to retain only unmasked tokens and pads sequences to uniform length.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Tensor of shape (batch_size, seq_len).\n",
    "            mask (torch.Tensor): Binary mask of the same shape as input_ids,\n",
    "                                where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
    "            padding_value (int, optional): Value to pad the sequences to uniform length. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Padded tensor of shape (batch_size, max_len), where max_len is the length of the longest retained sequence.\n",
    "        \"\"\"\n",
    "        if not attention:\n",
    "          #mask (batch , seq)\n",
    "          if self.choice_config != None:\n",
    "            if \"percent\" in self.choice_config: # top x percent\n",
    "              if self.choice_config[\"sample\"]:\n",
    "                percent = self.choice_config[\"percent\"]\n",
    "                k = int(mask.size(1) * percent)\n",
    "\n",
    "                # Apply softmax to create a probability distribution\n",
    "                mask_probs = torch.softmax(mask/ mask_temp, dim=1)\n",
    "\n",
    "                # Sample k indices for each row based on the probabilities\n",
    "                indices = torch.multinomial(mask_probs, num_samples=k, replacement=False)\n",
    "\n",
    "                # Create a new mask initialized to zeros\n",
    "                new_mask = torch.zeros_like(mask)\n",
    "\n",
    "                # Set the sampled indices to 1\n",
    "                new_mask.scatter_(1, indices, 1)\n",
    "\n",
    "                # Update the mask\n",
    "                mask = new_mask\n",
    "\n",
    "              else:\n",
    "                percent = self.choice_config[\"percent\"]\n",
    "                k = int(mask.size(1) * percent)\n",
    "                _, indices = torch.topk(mask, k, dim=1)\n",
    "                new_mask = torch.zeros_like(mask)\n",
    "                new_mask.scatter_(1, indices, 1)          # Set top-k values to 1\n",
    "                mask = new_mask\n",
    "               # print(mask)\n",
    "            elif \"threshold\" in self.choice_config: #above threshold\n",
    "              threshold = self.choice_config[\"threshold\"]\n",
    "              mask = (mask > threshold).float()\n",
    "            count = torch.sum(mask == 0).item()\n",
    "\n",
    "        updated_input_ids = []\n",
    "\n",
    "        for i in range(input_ids.size(0)):\n",
    "            # Use the mask to retain tokens from input_ids\n",
    "            retained_input_ids = input_ids[i][mask[i].bool()]\n",
    "            updated_input_ids.append(retained_input_ids)\n",
    "\n",
    "        # Find the maximum sequence length after masking\n",
    "        max_length = max(ids.size(0) for ids in updated_input_ids)\n",
    "\n",
    "        if max_length == 0:\n",
    "            #only return last tokens\n",
    "          return input_ids[:, -1].rehsape(-1, 1)\n",
    "\n",
    "\n",
    "          # Pad sequences to the maximum length\n",
    "        padded_input_ids = torch.full(\n",
    "              (input_ids.size(0), max_length),\n",
    "              padding_value,\n",
    "              dtype=input_ids.dtype,\n",
    "              device=input_ids.device\n",
    "          )\n",
    "\n",
    "        for i, ids in enumerate(updated_input_ids):\n",
    "            padded_input_ids[i, :ids.size(0)] = ids\n",
    "\n",
    "        return {\"input_ids\": padded_input_ids, \"count\":count, \"mask\":mask}\n",
    "\n",
    "\n",
    "  def forward(self, input_ids, attention_mask=None, labels = None, mask_temp= 1):\n",
    "    attention_mask = None\n",
    "    mask = self.forget_layer(input_ids)\n",
    "    data = self.apply_mask(input_ids, mask, mask_temp)\n",
    "    if \"input_ids\" in data:\n",
    "      input_ids = data[\"input_ids\"]\n",
    "      batch, seq_len = input_ids.size()\n",
    "    else:\n",
    "      input_embedings = data[\"embeddings\"]\n",
    "      batch, seq_len, _ = input_embedings.size()\n",
    "\n",
    "\n",
    "    if attention_mask != None:\n",
    "      attention_mask = self.apply_mask(attention_mask, mask, mask_temp)[\"input_ids\"]\n",
    "\n",
    "    if labels != None:\n",
    "      labels = labels[:, -1*seq_len:]\n",
    "\n",
    "    if \"input_ids\" in data:\n",
    "      output = self.base_llm_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels )\n",
    "    elif \"embeddings\" in data:\n",
    "      output = self.base_llm_model(inputs_embeds=input_embedings, attention_mask=attention_mask, labels=labels )\n",
    "\n",
    "    logits = output.logits  # Assuming 'output' is a named tuple or a dictionary\n",
    "\n",
    "    return_data = {\"logits\": logits, \"seq_len\": input_ids.size(1), \"loss\": output.loss}\n",
    "    if \"count\" in data:\n",
    "      return_data[\"count\"] = data[\"count\"]\n",
    "    if \"mask\" in data:\n",
    "      return_data[\"mask\"] = data[\"mask\"]\n",
    "    return return_data\n",
    "\n",
    "\n",
    "  def generate(self, input_ids, attention_mask =None, max_length=50, prob = True, temp= 1, mask_temp =1):\n",
    "      # Generate text by iteratively sampling next tokens\n",
    "      masks = []\n",
    "      for i in range(max_length):\n",
    "          # Run input through the model to get next token probabilities\n",
    "         # print(\"Mask\", i)\n",
    "          outputs = self.forward(input_ids,attention_mask = attention_mask ,mask_temp =1)\n",
    "          next_token_logits = outputs[\"logits\"][:, -1, :]  # Only use the logits for the last token\n",
    "          masks.append(outputs[\"mask\"])\n",
    "\n",
    "          if prob:\n",
    "            scaled_logits = next_token_logits / temp\n",
    "            next_token_probs = torch.softmax(scaled_logits, dim=-1)\n",
    "            next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "          else:\n",
    "          # Sample the next token\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "\n",
    "          if attention_mask != None:\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones_like(next_token)], dim=-1)\n",
    "\n",
    "          # Append the new token to the sequence\n",
    "          input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "      return input_ids,masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2w0ZG2uqUq1j"
   },
   "outputs": [],
   "source": [
    "class IdentityMask(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(IdentityMask, self).__init__()\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "      return torch.ones_like(input_ids, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcXEmRlxVECw"
   },
   "outputs": [],
   "source": [
    "class RandomForgetMask(nn.Module):\n",
    "  def __init__(self, forget_prob = .1):\n",
    "      super(RandomForgetMask, self).__init__()\n",
    "      self.forget_prob = forget_prob\n",
    "\n",
    "  def create_random_forgetting_mask(self, input_ids):\n",
    "      \"\"\"\n",
    "      Creates a mask for tokens to forget based on the forget rate.\n",
    "\n",
    "      Args:\n",
    "          input_ids (torch.Tensor): Tensor of shape (batch_size, seq_len).\n",
    "\n",
    "      Returns:\n",
    "          torch.Tensor: Binary mask of the same shape as input_ids,\n",
    "                        where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
    "      \"\"\"\n",
    "      batch_size, seq_len = input_ids.size()\n",
    "      num_tokens_to_forget = int((seq_len) * self.forget_prob)\n",
    "      masks = []\n",
    "\n",
    "      for i in range(batch_size):\n",
    "          # Generate random indices for tokens to forget\n",
    "          forget_indices = torch.randperm(seq_len)[:num_tokens_to_forget]\n",
    "          mask = torch.ones(seq_len, dtype=torch.bool, device=input_ids.device)\n",
    "          mask[forget_indices] = False  # Mark forgotten tokens as 0\n",
    "          masks.append(mask)\n",
    "\n",
    "      return torch.stack(masks)\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "      mask = self.create_random_forgetting_mask(input_ids)\n",
    "      return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiKnmMXWVGLz"
   },
   "outputs": [],
   "source": [
    "class MLPForgetMask(nn.Module):\n",
    "    def __init__(self, base_llm_model, hidden_dim=512):\n",
    "        \"\"\"\n",
    "        Initializes the MLPForgetMask module.\n",
    "\n",
    "        Args:\n",
    "            base_llm_model (nn.Module): The base language model (e.g., a transformer model).\n",
    "            tokenizer (PreTrainedTokenizer): The tokenizer corresponding to the language model.\n",
    "            threshold (float): Threshold for forgetting. Probabilities below this are set to 0 (forgotten).\n",
    "            hidden_dim (int): Hidden layer size for the MLP.\n",
    "        \"\"\"\n",
    "        super(MLPForgetMask, self).__init__()\n",
    "        self.embedding = base_llm_model.model.embed_tokens\n",
    "\n",
    "        # Retrieve the embedding dimension from the base model\n",
    "        embedding_dim = base_llm_model.config.hidden_size\n",
    "\n",
    "        # Define the MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Outputs a probability for retaining each token\n",
    "        )\n",
    "\n",
    "    def create_mlp_forgetting_mask(self, tokens):\n",
    "        \"\"\"\n",
    "        Creates a mask for tokens to retain/forget using the MLP.\n",
    "\n",
    "        Args:\n",
    "            tokens (torch.Tensor): Token embeddings of shape (batch_size, seq_len, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary mask of shape (batch_size, seq_len),\n",
    "                          where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = tokens.size()\n",
    "\n",
    "        # Flatten token embeddings for processing through MLP\n",
    "        tokens_flat = tokens.view(-1, embed_dim)  # Shape: (batch_size * seq_len, embedding_dim)\n",
    "\n",
    "        # Compute retain probabilities\n",
    "        retain_probs = self.mlp(tokens_flat).view(batch_size, seq_len)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        return retain_probs\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Generates the forgetting mask and applies it to the token embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs of shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Masked token embeddings.\n",
    "        \"\"\"\n",
    "        # Get token embeddings\n",
    "        tokens = self.embedding(input_ids)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Create mask\n",
    "        mask = self.create_mlp_forgetting_mask(tokens)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6u6Nr8UVJwB"
   },
   "outputs": [],
   "source": [
    "class RNNForgetMask(nn.Module):\n",
    "    def __init__(self, base_llm_model, threshold=0.5, hidden_dim=512, rnn_type=\"GRU\", bidirectional=True):\n",
    "        \"\"\"\n",
    "        Initializes the RNNForgetMask module.\n",
    "\n",
    "        Args:\n",
    "            base_llm_model (nn.Module): The base language model (e.g., a transformer model).\n",
    "            threshold (float): Threshold for forgetting. Probabilities below this are set to 0 (forgotten).\n",
    "            hidden_dim (int): Hidden layer size for the RNN.\n",
    "            rnn_type (str): Type of RNN to use (\"RNN\", \"LSTM\", or \"GRU\").\n",
    "        \"\"\"\n",
    "        super(RNNForgetMask, self).__init__()\n",
    "        self.embedding = base_llm_model.model.embed_tokens\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # Retrieve the embedding dimension from the base model\n",
    "        embedding_dim = base_llm_model.config.hidden_size\n",
    "\n",
    "        # Define the RNN layer\n",
    "        rnn_class = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[rnn_type]\n",
    "        self.rnn = rnn_class(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
    "\n",
    "        if bidirectional:\n",
    "            hidden_dim *= 2\n",
    "\n",
    "        # Linear layer to output probabilities\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Outputs a probability for retaining each token\n",
    "        )\n",
    "\n",
    "    def create_rnn_forgetting_mask(self, tokens):\n",
    "        \"\"\"\n",
    "        Creates a mask for tokens to retain/forget using the RNN.\n",
    "\n",
    "        Args:\n",
    "            tokens (torch.Tensor): Token embeddings of shape (batch_size, seq_len, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Binary mask of shape (batch_size, seq_len),\n",
    "                          where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, embed_dim = tokens.size()\n",
    "\n",
    "        # Pass tokens through RNN\n",
    "        rnn_output, _ = self.rnn(tokens)  # rnn_output: (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "\n",
    "\n",
    "        # Compute retain probabilities\n",
    "        retain_probs = self.output_layer(rnn_output).squeeze(-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "\n",
    "        return retain_probs\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Generates the forgetting mask and applies it to the token embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input token IDs of shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Masked token embeddings.\n",
    "        \"\"\"\n",
    "        # Get token embeddings\n",
    "        tokens = self.embedding(input_ids)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Create mask\n",
    "        mask = self.create_rnn_forgetting_mask(tokens)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_HL35bVBSSA"
   },
   "outputs": [],
   "source": [
    "def memory_check_and_empty():\n",
    "    \"\"\"Check GPU memory and clear cache only if necessary.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated_memory = torch.cuda.memory_allocated()\n",
    "        reserved_memory = torch.cuda.memory_reserved()\n",
    "        if reserved_memory - allocated_memory > 0.1 * reserved_memory:  # Threshold for unused memory\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h01c7CEIQ2VD"
   },
   "outputs": [],
   "source": [
    "def load_model(name,device ):\n",
    "  path = \"/content/drive/My Drive/llama/models/\" + name+\"/\"\n",
    "  if device != \"cuda\":\n",
    "    forget_layer = torch.load(path + \"forget.pt\", map_location=torch.device('cpu'))\n",
    "  else:\n",
    "    forget_layer = torch.load(path + \"forget.pt\")\n",
    "\n",
    "  base_llm_model = AutoModelForCausalLM.from_pretrained(path).to(device)\n",
    "  return base_llm_model, forget_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLLfwYl-XoD6"
   },
   "outputs": [],
   "source": [
    "#If using collab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ii18na1ver3M"
   },
   "outputs": [],
   "source": [
    "EXAMPLES = 10\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "val_data = dataset[\"validation\"].select(range(EXAMPLES))\n",
    "prompts = [prompt for prompt in val_data[\"text\"] if prompt.strip()]  # Filter out empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDvMR5yhRa1N"
   },
   "outputs": [],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cb3oKsywgKPh"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "#baseline_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqvsXPGOhjeX"
   },
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "import time\n",
    "def generate_responses(prompts, model, tokenizer, max_length=50, custom= False, verbose = True):\n",
    "    model.eval()\n",
    "    responses = []\n",
    "    masks = []\n",
    "    start_time = time.time()\n",
    "    for prompt in prompts:\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            if custom:\n",
    "              outputs, mask = model.generate(**inputs, max_length=max_length)\n",
    "            else:\n",
    "              outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        responses.append(response)\n",
    "        masks.append(mask)\n",
    "    total_time = time.time() - start_time\n",
    "    avg_time_per_response = total_time / len(prompts)\n",
    "    return responses, masks, avg_time_per_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0ZV-mnqiQ-Z"
   },
   "outputs": [],
   "source": [
    "# Perplexity calculation\n",
    "def calculate_perplexity(model, tokenizer, texts, batch_size=8, max_length=512, custom = False):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    notactive_tokens = 0\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            if custom:\n",
    "              loss= outputs[\"loss\"].item()\n",
    "              if \"count\" in outputs:\n",
    "                notactive_tokens += outputs[\"count\"]\n",
    "            else:\n",
    "              loss = outputs.loss.item()\n",
    "            total_loss += loss * inputs[\"input_ids\"].numel()\n",
    "            total_tokens += inputs[\"input_ids\"].numel()\n",
    "    ratio = (total_tokens -notactive_tokens)/total_tokens\n",
    "    perplexity = math.exp(total_loss / total_tokens)\n",
    "    return perplexity, ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-JMpZQ5KSr1Q"
   },
   "outputs": [],
   "source": [
    "name = \"ENTER NAME TO LOAD FOLDER\"\n",
    "base_llm_model, forget_layer = load_model(name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKqftfmJRh6J"
   },
   "outputs": [],
   "source": [
    "#Define how you want to chose values for mask, should be consistent with training for best results\n",
    "#Percent will retain top X%, and treshold will retain all vluaes with score > X\n",
    "#Set sample to true to sample from logits and use temp to adjust",
    "#choice_config = None\n",
    "#choice_config={\"percent\":.90, "sample":True}}\n",
    "#choice_config= {\"threshold\":.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMm4X4s9S4Db"
   },
   "outputs": [],
   "source": [
    "fine_tuned_model = Language_Model(base_llm_model, forget_layer, choice_config=choice_config).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdj5z5zcNJtU"
   },
   "outputs": [],
   "source": [
    "fine_tuned_responses, fine_tuned_time = generate_responses(prompts, fine_tuned_model, tokenizer, custom= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7a5hmlyC0LF"
   },
   "outputs": [],
   "source": [
    "fine_tuned_perplexity, fine_tuned_ratio = calculate_perplexity(fine_tuned_model, tokenizer, prompts, custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWjGT0Ws-Yvb"
   },
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "import time\n",
    "def generate_response(prompt, model, tokenizer, max_length=10, temp = 1, prob = True, mask_temp =1):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "       # print(inputs)\n",
    "        outputs, masks  = model.generate(**inputs, max_length=max_length, temp = 1, prob = True, mask_temp =1)\n",
    "       # print(outputs)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "      # Extract token IDs and mask\n",
    "        mask = masks[0]\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        print(\"inputs\", inputs)\n",
    "        print(\"mask\", mask)\n",
    "\n",
    "          # Identify retained tokens\n",
    "        retained_token_ids = input_ids[mask > .5]\n",
    "        retained_tokens = [tokenizer.decode([tid]) for tid in retained_token_ids]\n",
    "\n",
    "        print(\"Retained tokens\", retained_tokens)\n",
    "        fogotten_ids = input_ids[mask < .5]\n",
    "        fogotten_tokens = [tokenizer.decode([tid]) for tid in fogotten_ids]\n",
    "\n",
    "        print(\"Fogotten tokens\", fogotten_tokens)\n",
    "\n",
    "\n",
    "    print(max_length)\n",
    "    print(\"Response:\", response[-1*max_length: ] )\n",
    "    total_time = time.time() - start_time\n",
    "    return response, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEKbwhExR7IP"
   },
   "outputs": [],
   "source": [
    "p  = prompts[1][:len(prompts[1])//2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGrzbzgoUDI-"
   },
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O90ZS6GC-n6v"
   },
   "outputs": [],
   "source": [
    "#NOTE: Play with temp to find best values, lower temp is more consitent\n",
    "#prompt = \"The PlayStation[a] (abbreviated as PS, commonly known as the PS1/PS one or its codename PSX) is a home video game console developed and marketed by Sony Computer Entertainment. It\"\n",
    "prompt =  ' Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \\n'\n",
    "\n",
    "response, masks= generate_response(prompt, fine_tuned_model, tokenizer, max_length=100, temp = 0.1, prob = True, mask_temp=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKnoCaNuWf26"
   },
   "outputs": [],
   "source": [
    "!pip uninstall scipy numpy\n",
    "!pip install numpy==1.26.0\n",
    "!pip install scipy==1.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75EipJ15yZcI"
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgWMaLUSWiUG"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def calculate_bleu(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Calculate BLEU score for a set of references and hypotheses.\n",
    "\n",
    "    Args:\n",
    "        references (list of str): Ground truth text.\n",
    "        hypotheses (list of str): Generated text by the model.\n",
    "\n",
    "    Returns:\n",
    "        float: Average BLEU score.\n",
    "    \"\"\"\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        ref_tokens = ref.split()\n",
    "        hyp_tokens = hyp.split()\n",
    "        score = sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoothing_function)\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "    return sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0.0\n",
    "\n",
    "def calculate_rouge(references, hypotheses):\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores for a set of references and hypotheses.\n",
    "\n",
    "    Args:\n",
    "        references (list of str): Ground truth text.\n",
    "        hypotheses (list of str): Generated text by the model.\n",
    "\n",
    "    Returns:\n",
    "        dict: Average ROUGE-1, ROUGE-2, and ROUGE-L scores.\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        scores = scorer.score(ref, hyp)\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    return {key: sum(values) / len(values) if values else 0.0 for key, values in rouge_scores.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z35nXZayWrvf"
   },
   "outputs": [],
   "source": [
    "# Calculate BLEU and ROUGE\n",
    "fine_tuned_bleu = calculate_bleu(prompts, fine_tuned_responses)\n",
    "fine_tuned_rouge = calculate_rouge(prompts, fine_tuned_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InkeA_QfilhO"
   },
   "outputs": [],
   "source": [
    "print(f\"Fine-Tuned Model Perplexity: {fine_tuned_perplexity}\")\n",
    "print(f\"Fine-Tuned Model Ratio: {fine_tuned_ratio}\")\n",
    "print(f\"Fine-Tuned Avg Inference Time: {fine_tuned_time:.4f} seconds per response\")\n",
    "print(f\"Fine-Tuned Model BLEU: {fine_tuned_bleu}\")\n",
    "print(f\"Fine-Tuned Model ROUGE: {fine_tuned_rouge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoYiW_wxjsEA"
   },
   "outputs": [],
   "source": [
    "# Log results to WandB\n",
    "wandb.init(\n",
    "    project=\"llama_eval\",  # Replace with your WandB project name\n",
    "    config={\n",
    "        \"description\": \"Evaluation of fine-tuned vs baseline language models\",\n",
    "    }\n",
    ")\n",
    "logs = {\n",
    "    \"name\": name,\n",
    "    \"Fine-Tuned Model Perplexity\": fine_tuned_perplexity,\n",
    "    \"Fine-Tuned Avg Inference Time (s)\": fine_tuned_time,\n",
    "    \"Fine-Tuned Model BLEU\": fine_tuned_bleu,\n",
    "    \"Fine-Tuned Model ROUGE\": fine_tuned_rouge,\n",
    "}\n",
    "if choice_config != None:\n",
    "  logs.update(choice_config)\n",
    "wandb.log(logs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
