{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BLwPMqcpmxi"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWmRw_S-vpuR"
      },
      "source": [
        "Goal: At begining reduce and sequnce from n to b where b < n alllowing for savings of O(n^2-b^2) runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlCdw4Knh0OB"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install datasets\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzHC5BtLi6Yu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from huggingface_hub import login\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import wandb\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "from datasets import load_dataset, Dataset\n",
        "import shutil\n",
        "from time import sleep\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits\n",
        "import random\n",
        "from torch.nn.functional import binary_cross_entropy_with_logits\n",
        "import random\n",
        "login(token=\"ENTER TOKEN HERE\")\n",
        "wandb.login(key=\"ENTER KEY HERE\")\n",
        "device =  ('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLLfwYl-XoD6"
      },
      "outputs": [],
      "source": [
        "#If using collab\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz0T85X-BM9-"
      },
      "source": [
        "### **Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_random_forgetting_mask(input_ids, forget_prob):\n",
        "      \"\"\"\n",
        "      Creates a mask for tokens to forget based on the forget rate.\n",
        "\n",
        "      Args:\n",
        "          input_ids (torch.Tensor): Tensor of shape (batch_size, seq_len).\n",
        "\n",
        "      Returns:\n",
        "          torch.Tensor: Binary mask of the same shape as input_ids,\n",
        "                        where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
        "      \"\"\"\n",
        "      batch_size, seq_len = input_ids.size()\n",
        "      num_tokens_to_forget = int((seq_len) * forget_prob)\n",
        "      masks = []\n",
        "\n",
        "      for i in range(batch_size):\n",
        "          # Generate random indices for tokens to forget\n",
        "          forget_indices = torch.randperm(seq_len)[:num_tokens_to_forget]\n",
        "          mask = torch.ones(seq_len, dtype=torch.bool, device=input_ids.device)\n",
        "          mask[forget_indices] = False  # Mark forgotten tokens as 0\n",
        "          masks.append(mask)\n",
        "\n",
        "      return torch.stack(masks)"
      ],
      "metadata": {
        "id": "J5OAmtR0lsGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPfADRKtBPwZ"
      },
      "outputs": [],
      "source": [
        "def apply_mask( input_ids, mask, padding_value=0):\n",
        "      \"\"\"\n",
        "      Applies a forgetting mask to input_ids to retain only unmasked tokens and pads sequences to uniform length.\n",
        "\n",
        "      Args:\n",
        "          input_ids (torch.Tensor): Tensor of shape (batch_size, seq_len).\n",
        "          mask (torch.Tensor): Binary mask of the same shape as input_ids,\n",
        "                              where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
        "          padding_value (int, optional): Value to pad the sequences to uniform length. Defaults to 0.\n",
        "\n",
        "      Returns:\n",
        "          torch.Tensor: Padded tensor of shape (batch_size, max_len), where max_len is the length of the longest retained sequence.\n",
        "      \"\"\"\n",
        "      updated_input_ids = []\n",
        "\n",
        "      for i in range(input_ids.size(0)):\n",
        "          # Use the mask to retain tokens from input_ids\n",
        "          retained_input_ids = input_ids[i][mask[i].bool()]\n",
        "          updated_input_ids.append(retained_input_ids)\n",
        "\n",
        "\n",
        "      # Find the maximum sequence length after masking\n",
        "      max_length = max(ids.size(0) for ids in updated_input_ids)\n",
        "\n",
        "      if max_length == 0:\n",
        "        #only return last tokens\n",
        "        return input_ids[:, -1]\n",
        "\n",
        "\n",
        "        # Pad sequences to the maximum length\n",
        "      padded_input_ids = torch.full(\n",
        "            (input_ids.size(0), max_length),\n",
        "            padding_value,\n",
        "            dtype=input_ids.dtype,\n",
        "            device=input_ids.device\n",
        "        )\n",
        "\n",
        "      for i, ids in enumerate(updated_input_ids):\n",
        "          padded_input_ids[i, :ids.size(0)] = ids\n",
        "\n",
        "      return padded_input_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_HL35bVBSSA"
      },
      "outputs": [],
      "source": [
        "def memory_check_and_empty():\n",
        "    \"\"\"Check GPU memory and clear cache only if necessary.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        allocated_memory = torch.cuda.memory_allocated()\n",
        "        reserved_memory = torch.cuda.memory_reserved()\n",
        "        if reserved_memory - allocated_memory > 0.1 * reserved_memory:  # Threshold for unused memory\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels(mask, input_ids)"
      ],
      "metadata": {
        "id": "NwME20opnIZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpPC--nLlVly"
      },
      "source": [
        "### **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVYdN5hUB4OH"
      },
      "source": [
        "Models create Masks for input tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03JrKsMwBblG"
      },
      "outputs": [],
      "source": [
        "class IdentityMask(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(IdentityMask, self).__init__()\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "      return torch.ones_like(input_ids, dtype=torch.bool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjtaGMHVCtoQ"
      },
      "outputs": [],
      "source": [
        "class RandomForgetMask(nn.Module):\n",
        "  def __init__(self, forget_prob = .1):\n",
        "      super(RandomForgetMask, self).__init__()\n",
        "      self.forget_prob = forget_prob\n",
        "\n",
        "  def create_random_forgetting_mask(self, input_ids):\n",
        "      \"\"\"\n",
        "      Creates a mask for tokens to forget based on the forget rate.\n",
        "\n",
        "      Args:\n",
        "          input_ids (torch.Tensor): Tensor of shape (batch_size, seq_len).\n",
        "\n",
        "      Returns:\n",
        "          torch.Tensor: Binary mask of the same shape as input_ids,\n",
        "                        where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
        "      \"\"\"\n",
        "      batch_size, seq_len = input_ids.size()\n",
        "      num_tokens_to_forget = int((seq_len) * self.forget_prob)\n",
        "      masks = []\n",
        "\n",
        "      for i in range(batch_size):\n",
        "          # Generate random indices for tokens to forget\n",
        "          forget_indices = torch.randperm(seq_len)[:num_tokens_to_forget]\n",
        "          mask = torch.ones(seq_len, dtype=torch.bool, device=input_ids.device)\n",
        "          mask[forget_indices] = False  # Mark forgotten tokens as 0\n",
        "          masks.append(mask)\n",
        "\n",
        "      return torch.stack(masks)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "      mask = self.create_random_forgetting_mask(input_ids)\n",
        "      return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtxVBUW_D-kd"
      },
      "outputs": [],
      "source": [
        "class MLPForgetMask(nn.Module):\n",
        "    def __init__(self, base_llm_model, hidden_dim=512):\n",
        "        \"\"\"\n",
        "        Initializes the MLPForgetMask module.\n",
        "\n",
        "        Args:\n",
        "            base_llm_model (nn.Module): The base language model (e.g., a transformer model).\n",
        "            tokenizer (PreTrainedTokenizer): The tokenizer corresponding to the language model.\n",
        "            threshold (float): Threshold for forgetting. Probabilities below this are set to 0 (forgotten).\n",
        "            hidden_dim (int): Hidden layer size for the MLP.\n",
        "        \"\"\"\n",
        "        super(MLPForgetMask, self).__init__()\n",
        "        self.embedding = base_llm_model.model.embed_tokens\n",
        "\n",
        "        # Retrieve the embedding dimension from the base model\n",
        "        embedding_dim = base_llm_model.config.hidden_size\n",
        "\n",
        "        # Define the MLP\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()  # Outputs a probability for retaining each token\n",
        "        )\n",
        "\n",
        "    def create_mlp_forgetting_mask(self, tokens):\n",
        "        \"\"\"\n",
        "        Creates a mask for tokens to retain/forget using the MLP.\n",
        "\n",
        "        Args:\n",
        "            tokens (torch.Tensor): Token embeddings of shape (batch_size, seq_len, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Binary mask of shape (batch_size, seq_len),\n",
        "                          where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, embed_dim = tokens.size()\n",
        "\n",
        "        # Flatten token embeddings for processing through MLP\n",
        "        tokens_flat = tokens.view(-1, embed_dim)  # Shape: (batch_size * seq_len, embedding_dim)\n",
        "\n",
        "        # Compute retain probabilities\n",
        "        retain_probs = self.mlp(tokens_flat).view(batch_size, seq_len)  # Shape: (batch_size, seq_len)\n",
        "\n",
        "        return retain_probs\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Generates the forgetting mask and applies it to the token embeddings.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input token IDs of shape (batch_size, seq_len).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Masked token embeddings.\n",
        "        \"\"\"\n",
        "        # Get token embeddings\n",
        "        tokens = self.embedding(input_ids)  # Shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Create mask\n",
        "        mask = self.create_mlp_forgetting_mask(tokens)  # Shape: (batch_size, seq_len)\n",
        "\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc8XVpUtYYHp"
      },
      "outputs": [],
      "source": [
        "class CNNMLPForgetMask(nn.Module):\n",
        "    def __init__(self, base_llm_model, hidden_dim=512, cnn_hidden_dim=256, kernel_size=3):\n",
        "        \"\"\"\n",
        "        Initializes the CNNMLPForgetMask module.\n",
        "\n",
        "        Args:\n",
        "            base_llm_model (nn.Module): The base language model (e.g., a transformer model).\n",
        "            threshold (float): Threshold for forgetting. Probabilities below this are set to 0 (forgotten).\n",
        "            hidden_dim (int): Hidden layer size for the MLP.\n",
        "            cnn_hidden_dim (int): Number of output channels for the CNN.\n",
        "            kernel_size (int): Size of the convolutional kernel.\n",
        "        \"\"\"\n",
        "        super(CNNMLPForgetMask, self).__init__()\n",
        "        self.embedding = base_llm_model.model.embed_tokens\n",
        "\n",
        "        # Retrieve the embedding dimension from the base model\n",
        "        embedding_dim = base_llm_model.config.hidden_size\n",
        "\n",
        "        # Define the CNN layer\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=cnn_hidden_dim, kernel_size=kernel_size, padding=kernel_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(in_channels=cnn_hidden_dim, out_channels=embedding_dim, kernel_size=kernel_size, padding=kernel_size // 2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Define the MLP\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()  # Outputs a probability for retaining each token\n",
        "        )\n",
        "\n",
        "    def create_cnn_mlp_forgetting_mask(self, tokens):\n",
        "        \"\"\"\n",
        "        Creates a mask for tokens to retain/forget using CNN + MLP.\n",
        "\n",
        "        Args:\n",
        "            tokens (torch.Tensor): Token embeddings of shape (batch_size, seq_len, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Binary mask of shape (batch_size, seq_len),\n",
        "                          where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, embed_dim = tokens.size()\n",
        "\n",
        "        # Apply CNN: Convert to (batch_size, embed_dim, seq_len) for Conv1d\n",
        "        tokens_cnn_input = tokens.transpose(1, 2)  # Shape: (batch_size, embed_dim, seq_len)\n",
        "        tokens_cnn_output = self.cnn(tokens_cnn_input)  # Shape: (batch_size, embed_dim, seq_len)\n",
        "        tokens_processed = tokens_cnn_output.transpose(1, 2)  # Shape: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        # Flatten token embeddings for processing through MLP\n",
        "        tokens_flat = tokens_processed.view(-1, embed_dim)  # Shape: (batch_size * seq_len, embed_dim)\n",
        "\n",
        "        # Compute retain probabilities\n",
        "        retain_probs = self.mlp(tokens_flat).view(batch_size, seq_len)  # Shape: (batch_size, seq_len)\n",
        "\n",
        "\n",
        "        return retain_probs\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Generates the forgetting mask and applies it to the token embeddings.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input token IDs of shape (batch_size, seq_len).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Masked token embeddings.\n",
        "        \"\"\"\n",
        "        # Get token embeddings\n",
        "        tokens = self.embedding(input_ids)  # Shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Create mask using CNN + MLP\n",
        "        mask = self.create_cnn_mlp_forgetting_mask(tokens)  # Shape: (batch_size, seq_len)\n",
        "\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXpc8XYlUODU"
      },
      "outputs": [],
      "source": [
        "class RNNForgetMask(nn.Module):\n",
        "    def __init__(self, base_llm_model, threshold=0.5, hidden_dim=512, rnn_type=\"GRU\", bidirectional=True):\n",
        "        \"\"\"\n",
        "        Initializes the RNNForgetMask module.\n",
        "\n",
        "        Args:\n",
        "            base_llm_model (nn.Module): The base language model (e.g., a transformer model).\n",
        "            threshold (float): Threshold for forgetting. Probabilities below this are set to 0 (forgotten).\n",
        "            hidden_dim (int): Hidden layer size for the RNN.\n",
        "            rnn_type (str): Type of RNN to use (\"RNN\", \"LSTM\", or \"GRU\").\n",
        "        \"\"\"\n",
        "        super(RNNForgetMask, self).__init__()\n",
        "        self.embedding = base_llm_model.model.embed_tokens\n",
        "        self.threshold = threshold\n",
        "\n",
        "        # Retrieve the embedding dimension from the base model\n",
        "        embedding_dim = base_llm_model.config.hidden_size\n",
        "\n",
        "        # Define the RNN layer\n",
        "        rnn_class = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[rnn_type]\n",
        "        self.rnn = rnn_class(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "        if bidirectional:\n",
        "            hidden_dim *= 2\n",
        "\n",
        "        # Linear layer to output probabilities\n",
        "        self.output_layer = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()  # Outputs a probability for retaining each token\n",
        "        )\n",
        "\n",
        "    def create_rnn_forgetting_mask(self, tokens):\n",
        "        \"\"\"\n",
        "        Creates a mask for tokens to retain/forget using the RNN.\n",
        "\n",
        "        Args:\n",
        "            tokens (torch.Tensor): Token embeddings of shape (batch_size, seq_len, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Binary mask of shape (batch_size, seq_len),\n",
        "                          where 1 indicates retained tokens and 0 indicates forgotten tokens.\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, embed_dim = tokens.size()\n",
        "       # print('dfdfsfdsdfdfsfds')\n",
        "       # print(tokens.shape)\n",
        "\n",
        "        # Pass tokens through RNN\n",
        "        rnn_output, _ = self.rnn(tokens)  # rnn_output: (batch_size, seq_len, hidden_dim)\n",
        "\n",
        "      #  print(rnn_output.shape)\n",
        "\n",
        "        # Compute retain probabilities\n",
        "        retain_probs = self.output_layer(rnn_output).squeeze(-1)  # Shape: (batch_size, seq_len)\n",
        "\n",
        "\n",
        "        return retain_probs\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        Generates the forgetting mask and applies it to the token embeddings.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Input token IDs of shape (batch_size, seq_len).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Masked token embeddings.\n",
        "        \"\"\"\n",
        "        # Get token embeddings\n",
        "        tokens = self.embedding(input_ids)  # Shape: (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Create mask\n",
        "        mask = self.create_rnn_forgetting_mask(tokens)  # Shape: (batch_size, seq_len)\n",
        "\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNzwAYbMC87w"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57gjh9ESlahU"
      },
      "source": [
        "### **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb1qOcu4EoYG"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU4xxiZaId6g"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "EXAMPLES = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xvvdOGn4EFe"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "train_data = dataset[\"train\"].select(range(EXAMPLES))\n",
        "# Initialize tokenizer\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Function to preprocess examples\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "tokenized_dataset = train_data.map(preprocess_function, batched=True, remove_columns=['text'])\n",
        "tokenized_dataset.set_format(type='torch', columns=['input_ids'])\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_dataset, shuffle=True, batch_size=batch_size, num_workers=min(4, torch.get_num_threads()),\n",
        "        pin_memory=True,\n",
        "        prefetch_factor=2,  # Prefetch batches\n",
        "        persistent_workers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQujGvISP0YB"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "base_llm_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq2_pVINH4jw"
      },
      "outputs": [],
      "source": [
        "#Chose Mask Type\n",
        "#mask = IdentityMask().to(device)\n",
        "#mask = RandomForgetMask(forget_prob=.1).to(device)\n",
        "#mask = MLPForgetMask(base_llm_model).to(device)\n",
        "#mask = RNNForgetMask(base_llm_model, bidirectional=False).to(device)\n",
        "\n",
        "soft = False\n",
        "parameters = list(base_llm_model.parameters()) + list(mask.parameters())\n",
        "name = \"ENTER NAME HERE\"\n",
        "num_epochs = 5\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(parameters, lr=5e-5)\n",
        "total_steps = num_epochs * len(train_dataloader)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rreLP_8dG_ZA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def reinforce_model(base_model, forget_layer, forget_p,exploration_epochs,mask_batch, train_dataloader, optimizer, scheduler, num_epochs, device, model_name, save_dir,  batch_size=64):\n",
        "\n",
        "    wandb.init(project=\"forget_language_model_training\", config={\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset\": \"wikipedia\",\n",
        "    })\n",
        "\n",
        "    save_dir = os.path.join(save_dir, model_name)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    accumulation_steps = batch_size // train_dataloader.batch_size  # Gradient accumulation steps\n",
        "    total_steps = num_epochs * len(train_dataloader) // accumulation_steps\n",
        "    save_interval = (total_steps // 3) +1\n",
        "    step_count = 0\n",
        "    progress_bar = tqdm(total=total_steps, desc=\"Training\")\n",
        "\n",
        "    # Initialize mixed precision scaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    base_model.train()\n",
        "    forget_layer.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0  # Reset loss for each epoch\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "          input_ids = batch[\"input_ids\"].to(device)\n",
        "          explore = (random.random() <= exploration_epochs[epoch])\n",
        "\n",
        "          if explore:\n",
        "            random_mask = create_random_forgetting_mask(input_ids, forget_p).to(device)\n",
        "          else:\n",
        "            predicted_mask_logits = forget_layer(input_ids)\n",
        "\n",
        "          if explore:\n",
        "            mask = random_mask\n",
        "          else:\n",
        "            k = int(predicted_mask_logits.size(1) * (1-forget_p))\n",
        "            _, indices = torch.topk(predicted_mask_logits, k, dim=1)\n",
        "            new_mask = torch.zeros_like(predicted_mask_logits)\n",
        "            new_mask.scatter_(1, indices, 1)  # Set top-k values to 1\n",
        "            mask = new_mask\n",
        "\n",
        "\n",
        "          new_input_ids = apply_mask(input_ids, mask)\n",
        "          batchsize, seq_len = new_input_ids.size()\n",
        "          labels = input_ids[:, 1:]\n",
        "          labels = apply_mask(torch.cat([labels, torch.zeros((labels.size(0), 1), dtype=torch.long)], dim=1))\n",
        "\n",
        "          with autocast():\n",
        "            outputs = base_llm_model(input_ids=new_input_ids,  labels=labels)#attention_mask=attention_mask,\n",
        "            loss = outputs.loss / accumulation_steps\n",
        "            mloss = loss\n",
        "            if explore:\n",
        "              if True:\n",
        "                with torch.no_grad():\n",
        "                  input_ids = batch[\"input_ids\"].to(device)\n",
        "                  mask_data = []\n",
        "                  for _ in range(mask_batch//train_dataloader.batch_size):\n",
        "                    random_mask = create_random_forgetting_mask(input_ids, forget_p).to(device)\n",
        "                    new_input_ids = apply_mask(input_ids, random_mask)\n",
        "                    batchsize, seq_len = new_input_ids.size()\n",
        "                    labels = input_ids[:, 1:]\n",
        "                    labels = apply_mask(torch.cat([labels, torch.zeros((labels.size(0), 1), dtype=torch.long)], dim=1))\n",
        "                    outputs = base_llm_model(input_ids=new_input_ids,  labels=labels)\n",
        "                    mask_data.append([random_mask, outputs.loss.item()])\n",
        "                predicted_mask_logits = forget_layer(input_ids)\n",
        "                random_masks = torch.stack([item[0] for item in mask_data]).to(device)\n",
        "                losses = torch.tensor([item[1] for item in mask_data]).to(device)\n",
        "\n",
        "                loss_weights = torch.exp(-losses)  # Exponential decay to emphasize lower losses\n",
        "                normalized_weights = loss_weights / torch.sum(loss_weights)\n",
        "\n",
        "                expanded_predicted_mask_logits =  predicted_mask_logits.unsqueeze(0).repeat_interleave(mask_batch//train_dataloader.batch_size, dim=0)\n",
        "                bce_loss = binary_cross_entropy_with_logits(\n",
        "                    expanded_predicted_mask_logits, random_masks.float(), reduction=\"none\"\n",
        "                )\n",
        "                normalized_weights = normalized_weights.unsqueeze(1).repeat(1, 4)\n",
        "                mask_loss = (normalized_weights.unsqueeze(-1) * bce_loss).mean()/ accumulation_steps\n",
        "\n",
        "                mloss = loss + mask_loss\n",
        "            else:\n",
        "              mloss = loss\n",
        "\n",
        "          scaler.scale(mloss).backward()\n",
        "\n",
        "          if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_dataloader):\n",
        "              # Gradient update\n",
        "              scaler.step(optimizer)\n",
        "              scheduler.step()\n",
        "\n",
        "              scaler.update()\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              step_count += 1\n",
        "              progress_bar.update(1)\n",
        "              logs = {\"step_loss\": loss.item() * accumulation_steps, \"step\": step_count}\n",
        "              if explore:\n",
        "                logs[\"reinforce\"] = mask_loss.item() *accumulation_steps\n",
        "              wandb.log(logs)\n",
        "\n",
        "          epoch_loss += loss.item() * accumulation_steps  # Accumulate scaled loss\n",
        "          memory_check_and_empty()\n",
        "        # Epoch logging\n",
        "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "        wandb.log({\"epoch_loss\": avg_epoch_loss, \"epoch\": epoch})\n",
        "\n",
        "    # Final model save\n",
        "    save_path_forget = os.path.join(save_dir, f\"forget.pt\")\n",
        "    base_model.save_pretrained(save_dir)\n",
        "    torch.save(forget_layer, save_path_forget)\n",
        "    print(f\"Model saved\")\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxmyJFofIJW0"
      },
      "outputs": [],
      "source": [
        "#Rienforcement Learning with given list of probability to explore or not each batch during a epoch\n",
        "#forget_p = .10\n",
        "#mask_batch = 256\n",
        "#exploration_epochs = [.75, .50,.25, 0]\n",
        "#assert len(exploration_epochs) == num_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUsc74jKIERu"
      },
      "outputs": [],
      "source": [
        "#reinforce_model(base_llm_model, mask,forget_p, exploration_epochs,mask_batch,  train_dataloader,  optimizer, scheduler, num_epochs, device, name, \"/content/drive/My Drive/llama/models\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO-YTXsF4r-I"
      },
      "outputs": [],
      "source": [
        "def two_step_reinforce_model(base_model, forget_layer, forget_p,exploration_epochs,mask_batch, train_dataloader, optimizer, scheduler, num_epochs, device, model_name, save_dir,  batch_size=64):\n",
        "\n",
        "    wandb.init(project=\"forget_language_model_training\", config={\n",
        "        \"epochs\": num_epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
        "        \"model_name\": model_name,\n",
        "        \"dataset\": \"wikipedia\",\n",
        "    })\n",
        "\n",
        "    save_dir = os.path.join(save_dir, model_name)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    accumulation_steps = batch_size // train_dataloader.batch_size  # Gradient accumulation steps\n",
        "    total_steps = num_epochs * len(train_dataloader) // accumulation_steps\n",
        "    save_interval = (total_steps // 3) +1\n",
        "    step_count = 0\n",
        "    progress_bar = tqdm(total=total_steps, desc=\"Training\")\n",
        "\n",
        "    # Initialize mixed precision scaler\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    base_model.train()\n",
        "    forget_layer.train()\n",
        "    main_epoch = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0  # Reset loss for each epoch\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "          input_ids = batch[\"input_ids\"].to(device)\n",
        "          explore = exploration_epochs[epoch]\n",
        "\n",
        "          if explore:\n",
        "            with torch.no_grad():\n",
        "              input_ids = batch[\"input_ids\"].to(device)\n",
        "              mask_data = []\n",
        "              for _ in range(mask_batch//train_dataloader.batch_size):\n",
        "                random_mask = create_random_forgetting_mask(input_ids, forget_p).to(device)\n",
        "                new_input_ids = apply_mask(input_ids, random_mask)\n",
        "                batchsize, seq_len = new_input_ids.size()\n",
        "                labels = input_ids[:, 1:]\n",
        "                labels = apply_mask(torch.cat([labels, torch.zeros((labels.size(0), 1), dtype=torch.long)], dim=1))\n",
        "                outputs = base_llm_model(input_ids=new_input_ids,  labels=labels)\n",
        "                mask_data.append([random_mask, outputs.loss.item()])\n",
        "            predicted_mask_logits = forget_layer(input_ids)\n",
        "            random_masks = torch.stack([item[0] for item in mask_data]).to(device)\n",
        "            losses = torch.tensor([item[1] for item in mask_data]).to(device)\n",
        "\n",
        "            loss_weights = torch.exp(-losses)  # Exponential decay to emphasize lower losses\n",
        "            normalized_weights = loss_weights / torch.sum(loss_weights)\n",
        "\n",
        "            expanded_predicted_mask_logits =  predicted_mask_logits.unsqueeze(0).repeat_interleave(mask_batch//train_dataloader.batch_size, dim=0)\n",
        "            bce_loss = binary_cross_entropy_with_logits(\n",
        "                    expanded_predicted_mask_logits, random_masks.float(), reduction=\"none\"\n",
        "                )\n",
        "            normalized_weights = normalized_weights.unsqueeze(1).repeat(1, 4)\n",
        "            loss = (normalized_weights.unsqueeze(-1) * bce_loss).sum()/ accumulation_steps\n",
        "\n",
        "          else:\n",
        "            predicted_mask_logits = forget_layer(input_ids)\n",
        "            k = int(predicted_mask_logits.size(1) * (1-forget_p))\n",
        "            _, indices = torch.topk(predicted_mask_logits, k, dim=1)\n",
        "            new_mask = torch.zeros_like(predicted_mask_logits)\n",
        "            new_mask.scatter_(1, indices, 1)  # Set top-k values to 1\n",
        "            mask = new_mask\n",
        "\n",
        "            new_input_ids = apply_mask(input_ids, mask)\n",
        "            batchsize, seq_len = new_input_ids.size()\n",
        "            labels = input_ids[:, 1:]\n",
        "            labels = apply_mask(torch.cat([labels, torch.zeros((labels.size(0), 1), dtype=torch.long)], dim=1))\n",
        "\n",
        "            with autocast():\n",
        "              outputs = base_llm_model(input_ids=new_input_ids,  labels=labels)#attention_mask=attention_mask,\n",
        "              loss = outputs.loss / accumulation_steps\n",
        "\n",
        "          scaler.scale(loss).backward()\n",
        "\n",
        "          if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_dataloader):\n",
        "              # Gradient update\n",
        "              scaler.step(optimizer)\n",
        "\n",
        "              scaler.update()\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "\n",
        "              step_count += 1\n",
        "              progress_bar.update(1)\n",
        "              logs = { \"step\": step_count}\n",
        "              if explore:\n",
        "                logs[\"reinforce\"] = loss.item() *accumulation_steps\n",
        "              else:\n",
        "                scheduler.step()\n",
        "                logs[\"step_loss\"] = loss.item() *accumulation_steps\n",
        "              wandb.log(logs)\n",
        "\n",
        "          memory_check_and_empty()\n",
        "          if not explore:\n",
        "            epoch_loss += loss.item() * accumulation_steps  # Accumulate scaled loss\n",
        "\n",
        "        # Epoch logging\n",
        "        if not explore:\n",
        "          avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "          wandb.log({\"epoch_loss\": avg_epoch_loss, \"epoch\": main_epoch})\n",
        "          main_epoch +=1\n",
        "\n",
        "    # Final model save\n",
        "    save_path_forget = os.path.join(save_dir, f\"forget.pt\")\n",
        "    base_model.save_pretrained(save_dir)\n",
        "    torch.save(forget_layer, save_path_forget)\n",
        "    print(f\"Model saved\")\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsiTTrSB4zma"
      },
      "outputs": [],
      "source": [
        "#Rienforcement Learning with given list of explore or not at each epoch\n",
        "forget_p = .10\n",
        "mask_batch = 16\n",
        "exploration_epochs = [True, False ,True, False, False]\n",
        "assert len(exploration_epochs) == num_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD0zFgEV5ixm"
      },
      "outputs": [],
      "source": [
        "two_step_reinforce_model(base_llm_model, mask,forget_p, exploration_epochs,mask_batch,  train_dataloader,  optimizer, scheduler, num_epochs, device, name, \"/content/drive/My Drive/llama/models\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}